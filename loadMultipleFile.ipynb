{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "from skimage import io, transform\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading Multiple Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "traindir = '/work/test/Desktop/python/pytoch/dataset/'\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "                                 std=[1, 1, 1])\n",
    "\n",
    "train_dataset = dsets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(28),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter to divide train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide dataset into train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we dont want want to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=1, \n",
    "        num_workers=4, pin_memory=True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of test and validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(validation_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model with optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.batch_1=nn.BatchNorm2d(20)\n",
    "        self.conv2=nn.Conv2d(20, 50, 5, 1)\n",
    "        self.conv3=nn.Conv2d(50,60,5,1)\n",
    "        self.fc1 = nn.Linear(2*2*60, 500)\n",
    "        #self.batch_2=nn.BatchNorm1d(500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.batch_1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 2*2*60)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.batch_2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=CNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "criteria=nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 1.967\n",
      "[1,   400] loss: 2.245\n",
      "[1,   600] loss: 2.382\n",
      "[1,   800] loss: 2.168\n",
      "[1,  1000] loss: 1.815\n",
      "[2,   200] loss: 1.895\n",
      "[2,   400] loss: 1.996\n",
      "[2,   600] loss: 2.167\n",
      "[2,   800] loss: 2.186\n",
      "[2,  1000] loss: 2.028\n",
      "[3,   200] loss: 2.112\n",
      "[3,   400] loss: 2.127\n",
      "[3,   600] loss: 1.973\n",
      "[3,   800] loss: 2.082\n",
      "[3,  1000] loss: 1.940\n",
      "[4,   200] loss: 2.077\n",
      "[4,   400] loss: 2.019\n",
      "[4,   600] loss: 1.946\n",
      "[4,   800] loss: 1.908\n",
      "[4,  1000] loss: 2.219\n",
      "[5,   200] loss: 2.061\n",
      "[5,   400] loss: 1.910\n",
      "[5,   600] loss: 1.909\n",
      "[5,   800] loss: 1.966\n",
      "[5,  1000] loss: 2.132\n",
      "[6,   200] loss: 1.982\n",
      "[6,   400] loss: 1.988\n",
      "[6,   600] loss: 2.214\n",
      "[6,   800] loss: 2.275\n",
      "[6,  1000] loss: 1.914\n",
      "[7,   200] loss: 2.511\n",
      "[7,   400] loss: 2.051\n",
      "[7,   600] loss: 1.973\n",
      "[7,   800] loss: 2.028\n",
      "[7,  1000] loss: 1.562\n",
      "[8,   200] loss: 1.960\n",
      "[8,   400] loss: 2.017\n",
      "[8,   600] loss: 1.786\n",
      "[8,   800] loss: 2.272\n",
      "[8,  1000] loss: 2.279\n",
      "[9,   200] loss: 2.034\n",
      "[9,   400] loss: 2.178\n",
      "[9,   600] loss: 1.912\n",
      "[9,   800] loss: 1.926\n",
      "[9,  1000] loss: 1.915\n",
      "[10,   200] loss: 2.174\n",
      "[10,   400] loss: 2.263\n",
      "[10,   600] loss: 2.183\n",
      "[10,   800] loss: 1.846\n",
      "[10,  1000] loss: 1.892\n",
      "[11,   200] loss: 2.119\n",
      "[11,   400] loss: 1.910\n",
      "[11,   600] loss: 1.838\n",
      "[11,   800] loss: 1.802\n",
      "[11,  1000] loss: 2.246\n",
      "[12,   200] loss: 1.596\n",
      "[12,   400] loss: 1.955\n",
      "[12,   600] loss: 2.157\n",
      "[12,   800] loss: 2.066\n",
      "[12,  1000] loss: 1.943\n",
      "[13,   200] loss: 2.107\n",
      "[13,   400] loss: 1.712\n",
      "[13,   600] loss: 1.972\n",
      "[13,   800] loss: 1.829\n",
      "[13,  1000] loss: 1.866\n",
      "[14,   200] loss: 1.816\n",
      "[14,   400] loss: 2.017\n",
      "[14,   600] loss: 1.887\n",
      "[14,   800] loss: 1.976\n",
      "[14,  1000] loss: 2.113\n",
      "[15,   200] loss: 2.152\n",
      "[15,   400] loss: 2.095\n",
      "[15,   600] loss: 2.054\n",
      "[15,   800] loss: 1.946\n",
      "[15,  1000] loss: 1.915\n",
      "[16,   200] loss: 2.037\n",
      "[16,   400] loss: 1.926\n",
      "[16,   600] loss: 2.128\n",
      "[16,   800] loss: 2.205\n",
      "[16,  1000] loss: 1.948\n",
      "[17,   200] loss: 1.944\n",
      "[17,   400] loss: 1.974\n",
      "[17,   600] loss: 1.840\n",
      "[17,   800] loss: 1.938\n",
      "[17,  1000] loss: 1.994\n",
      "[18,   200] loss: 2.198\n",
      "[18,   400] loss: 2.240\n",
      "[18,   600] loss: 1.966\n",
      "[18,   800] loss: 1.944\n",
      "[18,  1000] loss: 1.702\n",
      "[19,   200] loss: 2.065\n",
      "[19,   400] loss: 1.806\n",
      "[19,   600] loss: 1.831\n",
      "[19,   800] loss: 1.935\n",
      "[19,  1000] loss: 2.056\n",
      "[20,   200] loss: 1.874\n",
      "[20,   400] loss: 1.810\n",
      "[20,   600] loss: 2.045\n",
      "[20,   800] loss: 2.129\n",
      "[20,  1000] loss: 1.727\n",
      "[21,   200] loss: 1.932\n",
      "[21,   400] loss: 1.900\n",
      "[21,   600] loss: 2.083\n",
      "[21,   800] loss: 1.636\n",
      "[21,  1000] loss: 1.938\n",
      "[22,   200] loss: 2.051\n",
      "[22,   400] loss: 2.038\n",
      "[22,   600] loss: 1.754\n",
      "[22,   800] loss: 1.757\n",
      "[22,  1000] loss: 1.904\n",
      "[23,   200] loss: 1.852\n",
      "[23,   400] loss: 2.026\n",
      "[23,   600] loss: 1.899\n",
      "[23,   800] loss: 1.696\n",
      "[23,  1000] loss: 2.165\n",
      "[24,   200] loss: 2.027\n",
      "[24,   400] loss: 1.886\n",
      "[24,   600] loss: 2.146\n",
      "[24,   800] loss: 1.625\n",
      "[24,  1000] loss: 1.912\n",
      "[25,   200] loss: 1.821\n",
      "[25,   400] loss: 1.587\n",
      "[25,   600] loss: 1.937\n",
      "[25,   800] loss: 2.131\n",
      "[25,  1000] loss: 1.791\n",
      "[26,   200] loss: 1.746\n",
      "[26,   400] loss: 1.680\n",
      "[26,   600] loss: 1.967\n",
      "[26,   800] loss: 1.999\n",
      "[26,  1000] loss: 1.902\n",
      "[27,   200] loss: 1.948\n",
      "[27,   400] loss: 2.003\n",
      "[27,   600] loss: 1.684\n",
      "[27,   800] loss: 1.594\n",
      "[27,  1000] loss: 1.851\n",
      "[28,   200] loss: 2.085\n",
      "[28,   400] loss: 1.977\n",
      "[28,   600] loss: 1.845\n",
      "[28,   800] loss: 1.800\n",
      "[28,  1000] loss: 1.602\n",
      "[29,   200] loss: 1.758\n",
      "[29,   400] loss: 1.955\n",
      "[29,   600] loss: 1.784\n",
      "[29,   800] loss: 1.859\n",
      "[29,  1000] loss: 1.526\n",
      "[30,   200] loss: 1.929\n",
      "[30,   400] loss: 1.862\n",
      "[30,   600] loss: 1.877\n",
      "[30,   800] loss: 1.942\n",
      "[30,  1000] loss: 1.516\n",
      "[31,   200] loss: 1.662\n",
      "[31,   400] loss: 1.816\n",
      "[31,   600] loss: 1.769\n",
      "[31,   800] loss: 1.909\n",
      "[31,  1000] loss: 1.773\n",
      "[32,   200] loss: 2.077\n",
      "[32,   400] loss: 1.947\n",
      "[32,   600] loss: 1.588\n",
      "[32,   800] loss: 1.724\n",
      "[32,  1000] loss: 1.609\n",
      "[33,   200] loss: 1.583\n",
      "[33,   400] loss: 1.844\n",
      "[33,   600] loss: 1.979\n",
      "[33,   800] loss: 1.679\n",
      "[33,  1000] loss: 1.532\n",
      "[34,   200] loss: 1.654\n",
      "[34,   400] loss: 1.850\n",
      "[34,   600] loss: 1.699\n",
      "[34,   800] loss: 1.895\n",
      "[34,  1000] loss: 1.829\n",
      "[35,   200] loss: 1.712\n",
      "[35,   400] loss: 1.894\n",
      "[35,   600] loss: 1.776\n",
      "[35,   800] loss: 1.733\n",
      "[35,  1000] loss: 1.684\n",
      "Accuracy of the network on the  test images: 71.111111 %\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "for epoch in range(35):\n",
    "    ave_loss=0.0\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        #print(input.shape,target)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(input)\n",
    "\n",
    "        loss=criteria(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ave_loss+=loss.item()\n",
    "        if i%200==199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, ave_loss / 100))\n",
    "            ave_loss = 0.0\n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "\n",
    "#test the model\n",
    "with torch.no_grad():\n",
    "    for inputs,labels in validation_loader:\n",
    "#         inputs=data['image']\n",
    "#         labels=data['label']\n",
    "        \n",
    "        inputs=torch.from_numpy(np.array(inputs))\n",
    "        labels=torch.from_numpy(np.array(labels))\n",
    "        outputs=model(inputs)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the  test images: %0.6f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
